# -*- coding: utf-8 -*-
"""NLI - Weston Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uNEVhb_jPtVUAZ8biUXOusNNAEIlF9C0

# 1. Pre-process data
"""

!pip install transformers==3.0.2
!pip install nlp

import gc
import os
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from nlp import load_dataset
from transformers import TFAutoModel, AutoTokenizer

import tensorflow as tf
from tensorflow.keras.utils import get_custom_objects
from tensorflow.keras import Input, Model, Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D, Flatten, Conv1D, MaxPooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import regularizers

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

np.random.seed(123)

# load data
train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')
submission = pd.read_csv('/content//sample_submission.csv')

"""a. For train data"""

print("Training data shape: ", train.shape)
print("Traning data samples: ")
train.head(3)

train['language'].value_counts()

train.language.unique()

print("Total number of languages in train data: ",len(train.language.unique()))

train.info()

"""b. For test data"""

print("Testing data shape: ", test.shape)
print("Testing data samples: ")
test.head(3)

test['language'].value_counts()

test.language.unique()

print("Total number of languages in test data: ",len(test.language.unique()))

test.info()

"""2. Perform data exploration

a. For train data
"""

train['label_str'] = train['label'].map({0 : "entailment", 1 : "neutral", 2 : "contradiction"})

plt.figure(figsize=(8,5))
sns.countplot(y ='label_str', data = train, alpha=.5, palette="muted")

plt.figure(figsize=(15,10))
sns.countplot(y ='language', hue = "label_str", data = train, alpha=.5, palette="muted")

"""# 3. Use sequence2sequence to build the model"""

for i in range(len(train['id'])):
  if train['language'][i] != "English":
    train = train.drop(i, axis = 0)
new_id = [i for i in range(len(train['id']))]
train.index = new_id
print("Length of data: ", len(train['id']))
train.head(10)

import spacy
import re
#instantiating English module
nlp = spacy.load('en_core_web_sm')
nlp.max_length = 10000
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
documents = []
stemmer = WordNetLemmatizer()
# combine two sentences into one
combine_sentences = []
for i in range(len(train['id'])):
    sentences = str(train['premise'][i]) + " " + str(train['hypothesis'][i])
    # Remove all the special characters
    document = re.sub(r'\W', ' ', str(sentences))
    # remove all single characters
    document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)
    # Remove single characters from the start
    document = re.sub(r'\^[a-zA-Z]\s+', ' ', document) 
    # Substituting multiple spaces with single space
    document = re.sub(r'\s+', ' ', document, flags=re.I)
    # Removing prefixed 'b'
    document = re.sub(r'^b\s+', '', document)
    # Converting to Lowercase
    document = document.lower()
    # Lemmatization
    document = document.split()
    document = [stemmer.lemmatize(word) for word in document]
    document = ' '.join(document)
    document = [document]
    combine_sentences.append(document)
train["combined"] = combine_sentences
train.head(5)

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
tfidfconverter = TfidfVectorizer(max_features=2)
# tfidfconverter = TfidfVectorizer()
vector_tfdif = []
for i in range(len(train["combined"])):
    input_value = train["combined"][i]
    # print(input_value, i)
    test_vec = tfidfconverter.fit_transform(input_value).toarray()
    vector_tfdif.append(test_vec[0])
print(len(vector_tfdif))

train["vector_tfdif"] = vector_tfdif
vector_tfdif_array = np.array(vector_tfdif)
print(vector_tfdif_array.shape)
train.head(10)

from keras.utils import to_categorical

X_tfdif, y_tfdif = train['vector_tfdif'].values.tolist(), train['label'].values.tolist()
X_tfdif = np.array(X_tfdif)
y_tfdif = np.array(y_tfdif)
y_tfdif = to_categorical(y_tfdif, 3)
print("Length of data: ", X_tfdif.shape)
print("Length of label: ", y_tfdif.shape)

X_train_tfdif, X_test_tfdif, Y_train_tfdif, Y_test_tfdif = train_test_split(X_tfdif, y_tfdif, test_size=0.2, random_state=1)
print("Shape of train data: ", X_train_tfdif.shape,Y_train_tfdif.shape)
print("Shape of test data: ", X_test_tfdif.shape,Y_test_tfdif.shape)

top_words = 1000
max_review_length = 500
embedding_vecor_length = 100
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(100))
model.add(Dense(3, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

EPOCHS = 15
BATCH_SIZE = 64
history_se2se = model.fit(X_train_tfdif, Y_train_tfdif, epochs=EPOCHS, batch_size=BATCH_SIZE)
# Final evaluation of the model
scores = model.evaluate(X_test_tfdif, Y_test_tfdif, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

plt.plot(history_se2se.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_se2se.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""* Using word2vec"""

from gensim.models import KeyedVectors
from gensim.models import Word2Vec

# load the google word2vec model
filename = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin'
model_word2vec = KeyedVectors.load_word2vec_format(filename, binary=True, limit = 300000)

# get new data for using word2vec
train_word2vec = train
train_word2vec.head(5)

#importing libraries
import spacy
import datetime
start_time = datetime.datetime.now()
#instantiating English module
nlp = spacy.load('en_core_web_sm')
nlp.max_length = 10000
word2vec_data = []
for i in range(len(train_word2vec["combined"])):
    #creating doc object containing our token features
    doc = nlp(train_word2vec["combined"][i][0])

    #Creating and updating our list of tokens using list comprehension 
    tokens = [token.text for token in doc]
    word2vec_data.append(tokens)
print("Length of word2vec_data: ", len(word2vec_data))
stop_time = datetime.datetime.now()
cost_time = (stop_time-start_time)
total_seconds = cost_time.total_seconds()
print("Cost time for using spacy step", total_seconds)

train_word2vec["spacy_data"] = word2vec_data
train_word2vec.head(5)

import numpy as np
word2vec_data_vec = []
for j in range(len(train_word2vec["spacy_data"])):
    vec = []
    for i in range(len(train_word2vec["spacy_data"][j])):
        if word2vec_data[j][i] in model_word2vec.vocab:
            a = np.array(model_word2vec.get_vector(word2vec_data[j][i]))
            vec.append(a)
    vec = np.array(vec)
    vec_average = np.average(vec, axis=0)
    word2vec_data_vec.append(vec_average)
print("Length of word2vec_data_vec: ", len(word2vec_data_vec))

nan_index = []
for i in range(len(word2vec_data_vec)):
  if str(word2vec_data_vec[i]) == "nan":
    print(word2vec_data_vec[i], i)
    nan_index.append(i)
print("All nan index: ", nan_index)

word2vec_data_vec = [i for j, i in enumerate(word2vec_data_vec) if j not in nan_index]

for i in range(len(nan_index)):
  train_word2vec = train_word2vec.drop(nan_index[i], axis = 0)

print("Length of data after: ", len(word2vec_data_vec), len(train_word2vec["spacy_data"]))

new_id = [i for i in range(len(train_word2vec['id']))]
train_word2vec.index = new_id

train_word2vec["word2vec_vector"] = word2vec_data_vec
train_word2vec.head(5)

from keras.utils import to_categorical

X_w2v, y_w2v = train_word2vec['word2vec_vector'].values.tolist(), train_word2vec['label'].values.tolist()
X_w2v = np.array(X_w2v)
y_w2v = np.array(y_w2v)
y_w2v = to_categorical(y_w2v, 3)
print("Length of data: ", X_w2v.shape)
print("Length of label: ", y_w2v.shape)

X_train_w2v, X_test_w2v, Y_train_w2v, Y_test_w2v = train_test_split(X_w2v, y_w2v, test_size=0.2, random_state=1)
print("Shape of train data: ", X_train_w2v.shape,Y_train_w2v.shape)
print("Shape of test data: ", X_test_w2v.shape,Y_test_w2v.shape)

top_words = 1000
max_review_length = 300
embedding_vecor_length = 300
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(100))
model.add(Dense(3, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

EPOCHS = 15
BATCH_SIZE = 64
history_se2se = model.fit(X_train_w2v, Y_train_w2v, epochs=EPOCHS, batch_size=BATCH_SIZE)
# Final evaluation of the model
scores = model.evaluate(X_test_w2v, Y_test_w2v, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))

plt.plot(history_se2se.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history_se2se.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""# 4. Use BERT to build the model

## *** Prepare data for training(BERT)
"""

# prepare data for training and testing
total_train = train.drop(columns=['id'])
shuffled_data = shuffle(total_train).reset_index(drop = True)
X, y = shuffled_data[['premise', 'hypothesis']].values.tolist(), shuffled_data['label']
print("Length of data: ", len(X))
print("Length of label: ", len(y))

x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=2020)
print("Length of train data: ", len(x_train))
print("Length of train label: ", len(y_train))

print("Length of valid data: ", len(x_valid))
print("Length of valid label: ", len(y_valid))

print("Example of x_train data: ", x_train[0])
print("Example of y_train data: ", y_train[0])

# Detect hardware, return appropriate distribution strategy
try:
    # TPU detection. No parameters necessary if TPU_NAME environment variable is
    # set: this is always the case on Kaggle.
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.
    strategy = tf.distribute.get_strategy()

print("REPLICAS: ", strategy.num_replicas_in_sync)

MODEL = 'joeddav/xlm-roberta-large-xnli'

EPOCHS = 4
MAXLEN = 120

BATCH_SIZE = 16 * strategy.num_replicas_in_sync

tokenizer = AutoTokenizer.from_pretrained(MODEL)
auto = tf.data.experimental.AUTOTUNE

def create_model(transformer_layer,learning_rate):
    input_ids = Input(shape = (MAXLEN,), dtype = tf.int32)
    #input_masks = Input(shape = (MAXLEN,), dtype = tf.int32)

    #insert roberta layer
    roberta = TFAutoModel.from_pretrained(transformer_layer)
    roberta = roberta([input_ids])[0]

    out = GlobalAveragePooling1D()(roberta)

    #add our softmax layer
    out = Dense(3, activation = 'softmax')(out)

    #assemble model and compile
    model = Model(inputs = input_ids, outputs = out)
    model.compile(
                    optimizer = Adam(lr = learning_rate), 
                    loss = 'sparse_categorical_crossentropy', 
                    metrics = ['accuracy'])
    return model


def tokeniZer(text,tokenizer):
    # tokenize
    encoded = tokenizer.batch_encode_plus(text, padding=True, max_length=MAXLEN, truncation=True)

    return np.array(encoded['input_ids'])

x_train_token = tokeniZer(x_train,tokenizer)
x_valid_token = tokeniZer(x_valid,tokenizer)
x_test_token  = tokeniZer(test[['premise', 'hypothesis']].values.tolist(),tokenizer)

print("x_train example after using xlm model: ", len(x_train_token[0]), '\n', x_train_token[0])

# datasets
def build_dataset(x, y, mode, batch_size):
    if mode == "train":
        dataset = (
            tf.data.Dataset
            .from_tensor_slices((x, y))
            .repeat()
            .shuffle(2048)
            .batch(batch_size)
            .prefetch(auto)
        )
    elif mode == "valid":
        dataset = (
            tf.data.Dataset
            .from_tensor_slices((x, y))
            .batch(BATCH_SIZE)
            .cache()
            .prefetch(auto)
        )
    elif mode == "test":
        dataset = (
            tf.data.Dataset
            .from_tensor_slices(x)
            .batch(BATCH_SIZE)
        )
    else:
        raise NotImplementedError
    return dataset

train_dataset = build_dataset(x_train_token, y_train, "train", BATCH_SIZE)
valid_dataset = build_dataset(x_valid_token, y_valid, "valid", BATCH_SIZE)
test_dataset  = build_dataset(x_test_token, None, "test", BATCH_SIZE)

print(train_dataset)
print(valid_dataset)
print(test_dataset)

def create_xlm(transformer_layer, random_seed, learning_rate):
    tf.keras.backend.clear_session()
    tf.random.set_seed(random_seed)
    with strategy.scope():
        model = create_model(transformer_layer,learning_rate)
    model.summary()    
    return model

Xlm = create_xlm(MODEL,1124234,1e-5)

callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2, 
                                              monitor = 'val_loss', 
                                              restore_best_weights = True, 
                                              mode = 'min')]

steps_per_epoch = len(x_train) // BATCH_SIZE
history_xlm = Xlm.fit(train_dataset,
                      validation_data=valid_dataset,
                      steps_per_epoch=steps_per_epoch,
                      epochs = EPOCHS)

# history_xlm = Xlm.fit(train_dataset,
#                       validation_data=valid_dataset,
#                       steps_per_epoch=steps_per_epoch,
#                       epochs = EPOCHS,
#                       callbacks = callbacks)

plt.plot(history_xlm.history['accuracy'])
plt.plot(history_xlm.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history_xlm.history['loss'])
plt.plot(history_xlm.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

#model predictions
predictions_xlm = Xlm.predict(test_dataset)
predictions = predictions_xlm
final = np.argmax(predictions, axis = 1)    

submission = pd.DataFrame()    
submission['id'] = test['id']
submission['prediction'] = final.astype(np.int32)

submission.to_csv('submission_BERT.csv', index = False)

submission.head()